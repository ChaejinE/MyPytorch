{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "q_Optimizer_State",
      "provenance": [],
      "authorship_tag": "ABX9TyPVrmjSp0JNCEmKBh58VBbZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChaejinE/MyPytorch/blob/main/PyTorch_Tips_Details/q_Optimizer_State.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_Lso9cbrABH"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNmQ2aoerMyM"
      },
      "source": [
        "# optimizer.state_dict()\n",
        "- model과 함꼐 더불어 저장하는 optimizer\n",
        "- checkpoint에 optimizer 저장 시 state.dict()를 이용해 저장한다.\n",
        "- 현재 사용하고 있는 optimizer의 상태 및 하이퍼 파라미터를 저장할 수 있다.\n",
        "- output은 state와 param_groups 다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cB-ZnorrwYU"
      },
      "source": [
        "```python\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
        "# 위와 같이 선언된 optimizer를 checkpoint['optimizer']에 저장하였다고 가정한다. (관습적으로 이 형식으로 저장한다.)\n",
        "\n",
        "optimizer_checkpoint = checkpoint['optimizer'] \n",
        "print(optimizer_checkpoint.keys())\n",
        "# dict_keys(['state', 'param_groups'])\n",
        "\n",
        "optimizer_checkpoint_states = optimizer_checkpoint['state']\n",
        "print(optimizer_checkpoint_state.keys())\n",
        "# dict_keys([140610494128064, 140610158587976, 140610158588048, ... , ])\n",
        "\n",
        "a_key = list(optimizer_checkpoint_states.keys())[0]\n",
        "optimizer_checkpoint_state = optimizer_checkpoint_states[a_key]\n",
        "print(optimizer_checkpoint_state.keys())\n",
        "# dict_keys(['step', 'exp_avg', 'exp_avg_sq'])\n",
        "\n",
        "optimizer_checkpoint_param_groups = optimizer_checkpoint['param_groups'] # list\n",
        "# optimizer_checkpoint_param_groups는 사용된 optimizer의 갯수 만큼 저장 됩니다.\n",
        "# 만약 1개의 optimizer를 사용하였다면 list의 원소 갯수는 1개 입니다.\n",
        "optimizer_checkpoint_param_group = optimizer_checkpoint_param_groups[0] \n",
        "print(optimizer_checkpoint_param_group .keys())\n",
        "#  dict_keys(['lr', 'betas', 'eps', 'weight_decay', 'amsgrad', 'initial_lr', 'params'])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4aEvbdBr0XW"
      },
      "source": [
        "- state (dict) : 현재 optimization state를 저장하고 있다.\n",
        "  - step(int)\n",
        "  - exp_avg (torch.Tensor)\n",
        "  - exp_avg_sq (torch.Tensor)\n",
        "- param_gropus (list) : 모든 parameter_group을 저장하고 있다. 최적화해야하는 텐서를 지정한다. 각 parameter_group 내용은 Adam(Optimizer)와 관련이 있다.\n",
        "  - lr\n",
        "  - betas\n",
        "  - eps\n",
        "  - weight_decay ... etc\n",
        "- 사전에 미리 학습된 weight를 이용해 학습을 재개할 때, model의 파라미터와 더불어 optimizer에 사용된 하이퍼 파라미터와 각 optimizer 알고리즘에서 사용하는 값들을 불러와 학습이 중단된 위치에서 그대로 학습이 가능해 지도록 한다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecNGKqjurGy0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}